# LLM-Coding
 An Overview of BERT and Language Models
Natural language processing (NLP) relies heavily on language models (LMs). They are made to recognise patterns in vast amounts of data and use those patterns to comprehend, produce, and modify natural language. By enabling strong contextual understanding of language, pre-trained language models like BERT (Bidirectional Encoder Representations from Transformers) have greatly increased natural language processing (NLP). They accomplish this by using transformer topologies, which, in contrast to conventional recurrent models, are able to capture word dependencies regardless of how far apart they are in a phrase.
Unlike unidirectional models such as GPT, BERT is bidirectional, meaning it reads text in both directions to improve context understanding. Because of its bidirectionality, BERT is able to take into account both the left and right context when creating a deep and rich representation of words. BERT's main benefit is its adaptability to different downstream tasks, like named entity recognition, sentiment analysis, and text categorisation. After the first pre-training phase, BERT is fine-tuned by training it on a smaller, task-specific dataset. This allows the model to adjust to the subtleties of the target task without requiring a new training.
Procedures for optimising the BERT model:

This project's overall methodology consisted of several stages, including evaluation, tokenisation, data preprocessing, and model fine-tuning. The Yelp review dataset was first loaded and preprocessed to change the sentiment labels into positive (1) or negative (0) categories in order to prepare them for binary classification. By making this modification, the task was made appropriate for binary sentiment categorisation. Preprocessing included cleaning up the text data of any discrepancies. Tokenisation, which involved using the BERT tokeniser to convert each review into a token that BERT could process, was the next crucial step. To guarantee uniform lengths (128 tokens), sentences were padded or trimmed to guarantee that the input format would be consistent for the model.
After preprocessing and tokenisation of the data, we adjusted the BERT model for sequence classification. Pre-trained weights from the 'bert-base-uncased' model were used to initialise the BERT model, and a classification head was added to provide two labels (positive and negative). The model's weights were modified using the AdamW optimiser, and the learning rate was regulated during the training epochs with the aid of a linear learning rate scheduler. The tokenised dataset was used to train the model in mini-batches, with the loss calculated each time. Gradient updates and optimiser steps were used in the training loop to adjust the model's parameters for precise sentiment prediction. When the model was tested on a different test set after training, it obtained a high accuracy of 97.40%, demonstrating its usefulness in categorising sentiments
Training environments:

The Yelp review dataset was utilised in this training environment to optimise a binary classification BERT-based model. Preprocessing the dataset made sure the labels were binary, designating a 1 for good reviews and a 0 for bad evaluations. The pre-trained BERT tokeniser was used to tokenise the text data, making sure that input sequences were padded or truncated to a maximum length of 128 tokens. As a result, the model could manage sequences of varying lengths while preserving consistency between batches. After that, the data was put into PyTorch DataLoaders, which allowed for effective batch processing for training and assessment.
The BertForSequenceClassification model from Hugging Face's transformers library was employed for this challenge; it was optimised for binary sentiment classification but was first pre-trained on tasks using general language. The training procedure had essential components such as the AdamW optimiser and a learning rate scheduler, which facilitated seamless convergence throughout the training stage. After three epochs of training, the model's loss steadily dropped, suggesting that the learning process was successful. The model had achieved a strong learning state when the last epoch had the lowest loss.

Outcomes and Model Performance
Three epochs were used to fine-tune the BERT model on a binary classification task. The loss decreased steadily from 0.4541 in the first epoch to 0.1811 in the third. This steady decline in loss indicates that the model picked up new information throughout training. A solid base was created by using BERT's pre-trained base model, which needed very little adjustment for the task at hand. To guarantee smooth convergence, the training was optimised with AdamW and a learning rate scheduler.
The model performed well in identifying the unseen data, achieving a test accuracy of 97.40% throughout the evaluation phase. This high accuracy shows how well the model generalises and how little overfitting there is. Based on these findings, the BERT model is a good fit for the binary classification problem; however, more fine-tuning or larger datasets can perhaps lead to even better performance.
 Concluding remarks
Using the Yelp Review Full dataset, this experiment showcased the effectiveness and methodology of optimising BERT for sentiment classification. When fine-tuned, pre-trained language models like as BERT can attain excellent performance levels with relatively minimal training data. They are highly effective for a wide range of natural language processing tasks. The project's outcomes demonstrate that BERT is a useful tool for applications like customer feedback analysis and review-based recommendation systems since it can efficiently categorise user evaluations into positive and negative attitudes.
References

Fine-tuning a masked language model - Hugging Face NLP Course n.d., huggingface.co, viewed 15 August 2023, <https://huggingface.co/learn/nlp-course/chapter7/3?fw=pt>.
Muller, B 2022, BERT 101 - State Of The Art NLP Model Explained, huggingface.co.

